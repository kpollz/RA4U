from serpapi import GoogleSearch
import json
from datetime import datetime
import re
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import time
import os
from dotenv import load_dotenv
import requests
from bs4 import BeautifulSoup
import urllib.parse
from pathlib import Path
import PyPDF2
import io

@dataclass
class Paper:
    """Class Ä‘á»ƒ represent má»™t paper tá»« Google Scholar"""
    title: str
    authors: str
    year: int
    venue: str
    citations: int
    snippet: str
    link: str
    paper_content: str = ""
    pdf_url: str = ""
    pdf_path: str = ""
    pdf_content: str = ""
    relevance_score: float = 0.0
    venue_score: float = 0.0
    recency_score: float = 0.0
    total_score: float = 0.0

class ScholarCrawler:
    """Class Ä‘á»ƒ crawl papers tá»« Google Scholar API cá»§a SerpApi"""
    
    def __init__(self, api_key: Optional[str] = None):
        if api_key:
            self.api_key = api_key
        else:
            load_dotenv()
            self.api_key = os.getenv('SERPAPI_API_KEY')
            if not self.api_key:
                raise ValueError("API key khÃ´ng tÃ¬m tháº¥y. Vui lÃ²ng set SERPAPI_API_KEY trong file .env hoáº·c truyá»n vÃ o constructor.")
        
        # Danh sÃ¡ch cÃ¡c venue/conference uy tÃ­n theo tá»«ng lÄ©nh vá»±c
        self.prestigious_venues = {
            'computer_science': [
                'ICML', 'NeurIPS', 'ICLR', 'AAAI', 'IJCAI', 'CVPR', 'ICCV', 'ECCV',
                'SIGIR', 'WWW', 'SIGKDD', 'SIGMOD', 'VLDB', 'ICDE', 'OSDI', 'SOSP',
                'PLDI', 'POPL', 'OOPSLA', 'ISCA', 'MICRO', 'ASPLOS', 'CHI', 'UIST',
                'ICSE', 'FSE', 'ASE', 'ISSTA', 'CCS', 'NDSS', 'USENIX Security',
                'S&P', 'CRYPTO', 'EUROCRYPT', 'ASIACRYPT', 'TCC', 'STOC', 'FOCS',
                'SODA', 'ICALP', 'ESA', 'SPAA', 'PODC', 'DISC'
            ],
            'biology': [
                'Nature', 'Science', 'Cell', 'PNAS', 'Nature Biotechnology',
                'Nature Medicine', 'Nature Genetics', 'Nature Neuroscience',
                'Nature Immunology', 'Nature Cell Biology', 'Molecular Cell',
                'Developmental Cell', 'Current Biology', 'EMBO Journal',
                'Journal of Cell Biology', 'PLoS Biology', 'eLife'
            ],
            'physics': [
                'Physical Review Letters', 'Nature Physics', 'Science',
                'Physical Review A', 'Physical Review B', 'Physical Review C',
                'Physical Review D', 'Physical Review E', 'Reviews of Modern Physics',
                'Annual Review of Nuclear and Particle Science', 'Physics Reports'
            ],
            'chemistry': [
                'Journal of the American Chemical Society', 'Angewandte Chemie',
                'Chemical Reviews', 'Nature Chemistry', 'Chemical Science',
                'Accounts of Chemical Research', 'Chemical Communications',
                'Organic Letters', 'Inorganic Chemistry', 'Analytical Chemistry'
            ]
        }
    
    def search_papers(self, query: str, num_results: int = 50) -> List[Paper]:
        """
        TÃ¬m kiáº¿m papers tá»« Google Scholar API
        
        Args:
            query: Research query Ä‘á»ƒ tÃ¬m kiáº¿m
            num_results: Sá»‘ lÆ°á»£ng results muá»‘n láº¥y
            
        Returns:
            List cÃ¡c Paper objects
        """
        papers = []
        start = 0
        results_per_page = 10
        
        while len(papers) < num_results:
            params = {
                "engine": "google_scholar",
                "q": query,
                "api_key": self.api_key,
                "start": start,
                "num": min(results_per_page, num_results - len(papers))
            }
            
            try:
                search = GoogleSearch(params)
                data = search.get_dict()
                
                if "organic_results" not in data:
                    print(f"KhÃ´ng tÃ¬m tháº¥y results cho query: {query}")
                    break
                
                for result in data["organic_results"]:
                    paper = self._parse_paper(result)
                    if paper:
                        papers.append(paper)
                
                # Kiá»ƒm tra xem cÃ²n results ná»¯a khÃ´ng
                if len(data["organic_results"]) < results_per_page:
                    break
                    
                start += results_per_page
                time.sleep(1)  # Rate limiting
                
            except Exception as e:
                print(f"Lá»—i khi gá»i API: {e}")
                break
        
        return papers[:num_results]
    
    def find_pdf_url(self, paper: Paper) -> str:
        """
        TÃ¬m PDF URL tá»« paper link
        
        Args:
            paper: Paper object
            
        Returns:
            PDF URL náº¿u tÃ¬m tháº¥y, empty string náº¿u khÃ´ng
        """
        if not paper.link:
            return ""
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            # Kiá»ƒm tra náº¿u link Ä‘Ã£ lÃ  PDF
            if paper.link.lower().endswith('.pdf'):
                return paper.link
            
            # Xá»­ lÃ½ arXiv links
            if 'arxiv.org' in paper.link:
                # Convert arXiv abstract URL to PDF URL
                if '/abs/' in paper.link:
                    pdf_url = paper.link.replace('/abs/', '/pdf/') + '.pdf'
                    return pdf_url
                elif '/pdf/' in paper.link:
                    return paper.link
            
            # Láº¥y ná»™i dung trang Ä‘á»ƒ tÃ¬m PDF links
            response = requests.get(paper.link, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # TÃ¬m cÃ¡c PDF links phá»• biáº¿n
            pdf_patterns = [
                # Direct PDF links
                'a[href$=".pdf"]',
                'a[href*=".pdf"]',
                # arXiv PDF links
                'a[href*="arxiv.org/pdf/"]',
                # ResearchGate PDF links
                'a[href*="researchgate.net"][href*=".pdf"]',
                # IEEE Xplore
                'a[href*="ieeexplore.ieee.org"][href*="stamp"]',
                # ACM Digital Library
                'a[href*="dl.acm.org"][href*=".pdf"]',
                # Springer
                'a[href*="link.springer.com"][href*=".pdf"]',
                # Generic PDF download buttons/links
                'a:contains("PDF")',
                'a:contains("Download PDF")',
                'a:contains("Full Text PDF")',
                'button:contains("PDF")'
            ]
            
            for pattern in pdf_patterns:
                elements = soup.select(pattern)
                for element in elements:
                    href = element.get('href', '')
                    if href:
                        # Convert relative URLs to absolute
                        if href.startswith('/'):
                            base_url = urllib.parse.urljoin(paper.link, '/')
                            href = urllib.parse.urljoin(base_url, href)
                        elif not href.startswith('http'):
                            href = urllib.parse.urljoin(paper.link, href)
                        
                        # Validate PDF URL
                        if self._is_valid_pdf_url(href):
                            return href
            
            # TÃ¬m trong meta tags
            meta_pdf = soup.find('meta', {'name': 'citation_pdf_url'})
            if meta_pdf and meta_pdf.get('content'):
                return meta_pdf['content']
            
            return ""
            
        except Exception as e:
            print(f"Lá»—i khi tÃ¬m PDF cho paper: {paper.title} - {e}")
            return ""
    
    def _is_valid_pdf_url(self, url: str) -> bool:
        """Kiá»ƒm tra URL cÃ³ pháº£i lÃ  PDF há»£p lá»‡ khÃ´ng"""
        try:
            # Basic URL validation
            if not url.startswith('http'):
                return False
            
            # Check if URL ends with .pdf or contains PDF indicators
            if url.lower().endswith('.pdf') or 'pdf' in url.lower():
                return True
            
            return False
        except:
            return False
    
    def download_pdf(self, paper: Paper, download_dir: str = "papers") -> bool:
        """
        Táº£i PDF cá»§a paper
        
        Args:
            paper: Paper object
            download_dir: ThÆ° má»¥c lÆ°u PDF
            
        Returns:
            True náº¿u táº£i thÃ nh cÃ´ng, False náº¿u khÃ´ng
        """
        if not paper.pdf_url:
            paper.pdf_url = self.find_pdf_url(paper)
        
        if not paper.pdf_url:
            print(f"KhÃ´ng tÃ¬m tháº¥y PDF cho paper: {paper.title}")
            return False
        
        try:
            # Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³
            Path(download_dir).mkdir(exist_ok=True)
            
            # Táº¡o tÃªn file an toÃ n
            safe_title = re.sub(r'[<>:"/\\|?*]', '_', paper.title)[:100]
            filename = f"{safe_title}_{paper.year}.pdf"
            filepath = Path(download_dir) / filename
            
            print(f"Äang táº£i PDF: {paper.title}")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(paper.pdf_url, headers=headers, timeout=30)
            response.raise_for_status()
            
            # Kiá»ƒm tra content type
            content_type = response.headers.get('content-type', '').lower()
            if 'pdf' not in content_type and not paper.pdf_url.lower().endswith('.pdf'):
                print(f"URL khÃ´ng tráº£ vá» PDF: {paper.pdf_url}")
                return False
            
            # LÆ°u file
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            paper.pdf_path = str(filepath)
            print(f"âœ… ÄÃ£ táº£i PDF: {filepath}")
            
            # Äá»c ná»™i dung PDF
            paper.pdf_content = self.extract_pdf_text(str(filepath))
            
            return True
            
        except Exception as e:
            print(f"âŒ Lá»—i khi táº£i PDF cho paper {paper.title}: {e}")
            return False
    
    def extract_pdf_text(self, pdf_path: str) -> str:
        """
        TrÃ­ch xuáº¥t text tá»« PDF file
        
        Args:
            pdf_path: ÄÆ°á»ng dáº«n Ä‘áº¿n PDF file
            
        Returns:
            Text content cá»§a PDF
        """
        try:
            text = ""
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
            
            # LÃ m sáº¡ch text
            text = re.sub(r'\s+', ' ', text).strip()
            
            # Giá»›i háº¡n Ä‘á»™ dÃ i
            return text[:50000] if len(text) > 50000 else text
            
        except Exception as e:
            print(f"Lá»—i khi Ä‘á»c PDF {pdf_path}: {e}")
            return ""
    
    def get_paper_content(self, paper: Paper) -> str:
        """
        Láº¥y ná»™i dung paper tá»« link (náº¿u cÃ³ thá»ƒ truy cáº­p)
        
        Args:
            paper: Paper object
            
        Returns:
            Ná»™i dung paper hoáº·c empty string náº¿u khÃ´ng láº¥y Ä‘Æ°á»£c
        """
        if not paper.link:
            return ""
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(paper.link, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # XÃ³a script vÃ  style tags
            for script in soup(["script", "style"]):
                script.decompose()
            
            # Láº¥y text content
            text = soup.get_text()
            
            # LÃ m sáº¡ch text
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            # Giá»›i háº¡n Ä‘á»™ dÃ i (10000 characters)
            return text[:10000] if len(text) > 10000 else text
            
        except Exception as e:
            print(f"KhÃ´ng thá»ƒ láº¥y ná»™i dung cho paper: {paper.title} - Lá»—i: {e}")
            return ""
    
    def _parse_paper(self, result: Dict[str, Any]) -> Paper:
        """Parse má»™t result tá»« API thÃ nh Paper object"""
        try:
            title = result.get("title", "")
            
            # Parse publication info
            pub_info = result.get("publication_info", {})
            summary = pub_info.get("summary", "")
            
            # Extract year tá»« summary
            year_match = re.search(r'\b(19|20)\d{2}\b', summary)
            year = int(year_match.group()) if year_match else datetime.now().year
            
            # Extract authors vÃ  venue
            authors = ""
            venue = ""
            if summary:
                parts = summary.split(" - ")
                if len(parts) >= 2:
                    authors = parts[0]
                    venue = parts[-1] if len(parts) > 2 else parts[1]
            
            # Get citations
            citations = 0
            inline_links = result.get("inline_links", {})
            cited_by = inline_links.get("cited_by", {})
            if cited_by:
                citations = cited_by.get("total", 0)
            
            snippet = result.get("snippet", "")
            link = result.get("link", "")
            
            return Paper(
                title=title,
                authors=authors,
                year=year,
                venue=venue,
                citations=citations,
                snippet=snippet,
                link=link
            )
            
        except Exception as e:
            print(f"Lá»—i khi parse paper: {e}")
            return None
    
    def calculate_relevance_score(self, paper: Paper, query: str) -> float:
        """
        TÃ­nh Ä‘iá»ƒm Ä‘á»™ liÃªn quan dá»±a trÃªn title vÃ  snippet
        
        Args:
            paper: Paper object
            query: Original search query
            
        Returns:
            Relevance score tá»« 0-1
        """
        query_terms = query.lower().split()
        text = (paper.title + " " + paper.snippet).lower()
        
        # Äáº¿m sá»‘ terms xuáº¥t hiá»‡n
        matches = sum(1 for term in query_terms if term in text)
        base_score = matches / len(query_terms)
        
        # Bonus cho exact phrase matches
        if query.lower() in text:
            base_score += 0.2
        
        # Normalize citations (log scale)
        citation_bonus = min(0.3, paper.citations / 1000) if paper.citations > 0 else 0
        
        return min(1.0, base_score + citation_bonus)
    
    def calculate_venue_score(self, paper: Paper, field: str = 'computer_science') -> float:
        """
        TÃ­nh Ä‘iá»ƒm venue dá»±a trÃªn danh sÃ¡ch prestigious venues
        
        Args:
            paper: Paper object
            field: Research field Ä‘á»ƒ chá»n venue list phÃ¹ há»£p
            
        Returns:
            Venue score tá»« 0-1
        """
        if field not in self.prestigious_venues:
            field = 'computer_science'  # Default
        
        prestigious_list = self.prestigious_venues[field]
        venue_lower = paper.venue.lower()
        
        for venue in prestigious_list:
            if venue.lower() in venue_lower:
                return 1.0
        
        # Partial matching cho journal names
        for venue in prestigious_list:
            venue_words = venue.lower().split()
            if any(word in venue_lower for word in venue_words if len(word) > 3):
                return 0.7
        
        # Bonus cho high citation venues
        if paper.citations > 100:
            return 0.5
        elif paper.citations > 50:
            return 0.3
        
        return 0.1
    
    def calculate_recency_score(self, paper: Paper) -> float:
        """
        TÃ­nh Ä‘iá»ƒm Ä‘á»™ gáº§n Ä‘Ã¢y
        
        Args:
            paper: Paper object
            
        Returns:
            Recency score tá»« 0-1
        """
        current_year = datetime.now().year
        years_diff = current_year - paper.year
        
        if years_diff <= 1:
            return 1.0
        elif years_diff <= 3:
            return 0.8
        elif years_diff <= 5:
            return 0.6
        elif years_diff <= 10:
            return 0.4
        else:
            return 0.2
    
    def rerank_papers(self, papers: List[Paper], query: str, field: str = 'computer_science',
                     weights: Dict[str, float] = None) -> List[Paper]:
        """
        Rerank papers theo cÃ¡c tiÃªu chÃ­
        
        Args:
            papers: List cÃ¡c Paper objects
            query: Original search query
            field: Research field
            weights: Trá»ng sá»‘ cho tá»«ng tiÃªu chÃ­
            
        Returns:
            List papers Ä‘Ã£ Ä‘Æ°á»£c rerank
        """
        if weights is None:
            weights = {
                'relevance': 0.4,
                'venue': 0.35,
                'recency': 0.25
            }
        
        # TÃ­nh scores cho tá»«ng paper
        for paper in papers:
            paper.relevance_score = self.calculate_relevance_score(paper, query)
            paper.venue_score = self.calculate_venue_score(paper, field)
            paper.recency_score = self.calculate_recency_score(paper)
            
            # TÃ­nh total score
            paper.total_score = (
                paper.relevance_score * weights['relevance'] +
                paper.venue_score * weights['venue'] +
                paper.recency_score * weights['recency']
            )
        
        # Sort theo total score
        return sorted(papers, key=lambda p: p.total_score, reverse=True)
    
    def search_and_rank(self, query: str, field: str = 'computer_science', 
                       num_results: int = 50, weights: Dict[str, float] = None, 
                       fetch_content: bool = True) -> List[Paper]:
        """
        Main function: Search vÃ  rerank papers
        
        Args:
            query: Research query
            field: Research field
            num_results: Sá»‘ lÆ°á»£ng results
            weights: Custom weights
            fetch_content: CÃ³ láº¥y ná»™i dung paper khÃ´ng
            
        Returns:
            List papers Ä‘Ã£ Ä‘Æ°á»£c ranked
        """
        print(f"Äang tÃ¬m kiáº¿m papers cho query: '{query}'...")
        papers = self.search_papers(query, num_results)
        
        
        
        print(f"TÃ¬m tháº¥y {len(papers)} papers. Äang rerank...")
        ranked_papers = self.rerank_papers(papers, query, field, weights)
        
        return ranked_papers
    
    def download_top_papers(self, papers: List[Paper], top_k: int = 5, download_dir: str = "papers") -> List[Paper]:
        """
        Táº£i PDF cá»§a top K papers
        
        Args:
            papers: List papers Ä‘Ã£ Ä‘Æ°á»£c ranked
            top_k: Sá»‘ lÆ°á»£ng papers cáº§n táº£i
            download_dir: ThÆ° má»¥c lÆ°u PDF
            
        Returns:
            List papers vá»›i PDF Ä‘Ã£ táº£i
        """
        top_papers = papers[:top_k]
        successful_downloads = []
        
        print(f"\nğŸ”½ ÄANG Táº¢I TOP {top_k} PAPERS...")
        print("=" * 50)
        
        for i, paper in enumerate(top_papers, 1):
            print(f"\n[{i}/{top_k}] {paper.title}")
            
            # TÃ¬m PDF URL
            print("ğŸ” Äang tÃ¬m PDF link...")
            paper.pdf_url = self.find_pdf_url(paper)
            
            if paper.pdf_url:
                print(f"âœ… TÃ¬m tháº¥y PDF: {paper.pdf_url}")
                
                # Táº£i PDF
                success = self.download_pdf(paper, download_dir)
                if success:
                    successful_downloads.append(paper)
                    print(f"ğŸ“„ PDF Content Length: {len(paper.pdf_content)} characters")
                else:
                    print("âŒ KhÃ´ng thá»ƒ táº£i PDF")
            else:
                print("âŒ KhÃ´ng tÃ¬m tháº¥y PDF link")
            
            print("-" * 30)
            time.sleep(2)  # Rate limiting
        
        print(f"\nâœ… ÄÃ£ táº£i thÃ nh cÃ´ng {len(successful_downloads)}/{top_k} papers")
        return successful_downloads
    
    def print_results(self, papers: List[Paper], top_k: int = 10):
        """In káº¿t quáº£ top papers"""
        print(f"\n=== TOP {min(top_k, len(papers))} PAPERS ===\n")
        
        for i, paper in enumerate(papers[:top_k], 1):
            print(f"{i}. {paper.title}")
            print(f"   Authors: {paper.authors}")
            print(f"   Venue: {paper.venue} ({paper.year})")
            print(f"   Citations: {paper.citations}")
            print(f"   Content Length: {len(paper.paper_content)} characters")
            if paper.pdf_url:
                print(f"   ğŸ“„ PDF URL: {paper.pdf_url}")
            if paper.pdf_path:
                print(f"   ğŸ’¾ PDF Downloaded: {paper.pdf_path}")
                print(f"   ğŸ“– PDF Content Length: {len(paper.pdf_content)} characters")
            print(f"   Scores - Relevance: {paper.relevance_score:.3f}, "
                  f"Venue: {paper.venue_score:.3f}, "
                  f"Recency: {paper.recency_score:.3f}")
            print(f"   Total Score: {paper.total_score:.3f}")
            print(f"   Link: {paper.link}")
            if paper.pdf_content:
                print(f"   PDF Preview: {paper.pdf_content[:200]}...")
            elif paper.paper_content:
                print(f"   Content Preview: {paper.paper_content[:200]}...")
            print("-" * 80)

def main():
    """Demo function"""
    # Sá»­ dá»¥ng API key tá»« .env file
    crawler = ScholarCrawler()
    
    # Example usage
    query = "machine learning natural language processing"
    field = "computer_science"
    
    # Custom weights (optional)
    custom_weights = {
        'relevance': 0.5,    # TÄƒng trá»ng sá»‘ Ä‘á»™ liÃªn quan
        'venue': 0.3,       # Giáº£m trá»ng sá»‘ venue
        'recency': 0.2      # Giáº£m trá»ng sá»‘ thá»i gian
    }
    
    try:
        # TÃ¬m kiáº¿m vÃ  rank papers (khÃ´ng láº¥y content Ä‘á»ƒ nhanh hÆ¡n)
        papers = crawler.search_and_rank(
            query=query,
            field=field,
            num_results=30,
            weights=custom_weights,
            fetch_content=False  # KhÃ´ng láº¥y content trang web Ä‘á»ƒ táº­p trung vÃ o PDF
        )
        
        # Hiá»ƒn thá»‹ káº¿t quáº£ ban Ä‘áº§u
        crawler.print_results(papers, top_k=10)
        
        # Táº£i PDF cho top 5 papers
        downloaded_papers = crawler.download_top_papers(papers, top_k=5, download_dir="papers")
        
        # Hiá»ƒn thá»‹ káº¿t quáº£ sau khi táº£i PDF
        if downloaded_papers:
            print(f"\nğŸ“š Káº¾T QUáº¢ SAU KHI Táº¢I PDF:")
            crawler.print_results(downloaded_papers, top_k=len(downloaded_papers))
        
        # Export to JSON vá»›i thÃ´ng tin PDF
        export_data = []
        for paper in papers[:10]:
            export_data.append({
                'title': paper.title,
                'authors': paper.authors,
                'venue': paper.venue,
                'year': paper.year,
                'citations': paper.citations,
                'snippet': paper.snippet,
                'paper_content': paper.paper_content,
                'pdf_url': paper.pdf_url,
                'pdf_path': paper.pdf_path,
                'pdf_content': paper.pdf_content[:1000] if paper.pdf_content else "",  # Chá»‰ láº¥y 1000 kÃ½ tá»± Ä‘áº§u
                'total_score': paper.total_score,
                'relevance_score': paper.relevance_score,
                'venue_score': paper.venue_score,
                'recency_score': paper.recency_score,
                'link': paper.link
            })
        
        with open('ranked_papers_with_pdf.json', 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… ÄÃ£ export {len(export_data)} papers vÃ o file 'ranked_papers_with_pdf.json'")
        
        # Thá»‘ng kÃª
        pdf_count = sum(1 for p in papers[:10] if p.pdf_path)
        print(f"\nğŸ“Š THá»NG KÃŠ:")
        print(f"   â€¢ Tá»•ng papers tÃ¬m tháº¥y: {len(papers)}")
        print(f"   â€¢ Papers cÃ³ PDF: {pdf_count}/10")
        print(f"   â€¢ ThÆ° má»¥c lÆ°u PDF: ./papers/")
        
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")

if __name__ == "__main__":
    main()
